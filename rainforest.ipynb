{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8sALVSeZbDI"
      },
      "source": [
        "### We can construct a mosaic of nearby tiles using this method: https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/discussion/36738"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as K\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, Dense, Activation, BatchNormalization, Flatten\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yztgcSUwSDwZ"
      },
      "source": [
        "# Preprocess data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtain Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'haze', 1: 'primary', 2: 'agriculture', 3: 'clear', 4: 'water', 5: 'habitation', 6: 'road', 7: 'cultivation', 8: 'slash_burn', 9: 'cloudy', 10: 'partly_cloudy', 11: 'conventional_mine', 12: 'bare_ground', 13: 'artisinal_mine', 14: 'blooming', 15: 'selective_logging', 16: 'blow_down'}\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv('data/train_v2.csv')\n",
        "\n",
        "curr_count = 0\n",
        "unique_labels = {}\n",
        "multihot = {}\n",
        "for line in train_data['tags'].values:\n",
        "    for label in line.split():\n",
        "        if label not in unique_labels:\n",
        "            unique_labels[label] = curr_count\n",
        "            curr_count += 1\n",
        "\n",
        "mapping = {}\n",
        "\n",
        "for k, v in unique_labels.items():\n",
        "    mapping[k] = np.zeros(len(unique_labels))\n",
        "    mapping[k][v] = 1\n",
        "\n",
        "n_labels = len(mapping)\n",
        "label2name = {v: k for k, v in unique_labels.items()}\n",
        "\n",
        "print(label2name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Head of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>haze primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>agriculture clear primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>clear primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>clear primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>agriculture clear habitation primary road</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>train_5</td>\n",
              "      <td>haze primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>train_6</td>\n",
              "      <td>agriculture clear cultivation primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>train_7</td>\n",
              "      <td>haze primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>train_8</td>\n",
              "      <td>agriculture clear cultivation primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>train_9</td>\n",
              "      <td>agriculture clear cultivation primary road</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  image_name                                         tags\n",
              "0    train_0                                 haze primary\n",
              "1    train_1              agriculture clear primary water\n",
              "2    train_2                                clear primary\n",
              "3    train_3                                clear primary\n",
              "4    train_4    agriculture clear habitation primary road\n",
              "5    train_5                           haze primary water\n",
              "6    train_6  agriculture clear cultivation primary water\n",
              "7    train_7                                 haze primary\n",
              "8    train_8        agriculture clear cultivation primary\n",
              "9    train_9   agriculture clear cultivation primary road"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head(n = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Auxiliary Function for multi-hotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multihot(label):\n",
        "    z = np.zeros(n_labels)\n",
        "    tokens = label.split(' ')\n",
        "    for k in range(len(tokens)):\n",
        "        z += mapping[tokens[k]]\n",
        "\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00 % complete\n",
            "5.00 % complete\n",
            "10.00 % complete\n",
            "15.00 % complete\n",
            "20.00 % complete\n",
            "25.00 % complete\n",
            "30.00 % complete\n",
            "35.00 % complete\n",
            "40.00 % complete\n",
            "45.00 % complete\n",
            "50.00 % complete\n",
            "55.00 % complete\n",
            "60.00 % complete\n",
            "65.00 % complete\n",
            "70.00 % complete\n",
            "75.00 % complete\n",
            "80.00 % complete\n",
            "85.00 % complete\n",
            "90.00 % complete\n",
            "95.00 % complete\n"
          ]
        }
      ],
      "source": [
        "# first pass, construct a list of image strips\n",
        "\n",
        "train_path = 'data/train-jpg/'\n",
        "\n",
        "num_images = len(os.listdir(train_path))\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "num_jpgs = 1000\n",
        "for iter, file in enumerate(os.listdir(train_path)[:num_jpgs]):\n",
        "    X.append(train_path + file)\n",
        "    y.append(train_data['tags'][iter])\n",
        "\n",
        "    if iter % (0.05 * num_jpgs) == 0:\n",
        "        print(f\"{(100 * iter / num_jpgs):.2f} % complete\")\n",
        "\n",
        "# X_np = np.array(X) / 255\n",
        "\n",
        "y_np = np.zeros(shape = (num_jpgs, n_labels))\n",
        "\n",
        "\n",
        "for i, label in enumerate(y):\n",
        "    y_np[i] = multihot(label)\n",
        "\n",
        "y = y_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into a test and train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_split = 0.2\n",
        "\n",
        "indices = np.random.permutation(len(X))\n",
        "train_length = math.floor(indices.shape[0] * (1 - validation_split))\n",
        "train_indices, test_indices = indices[0:train_length], indices[train_length:]\n",
        "\n",
        "X_train, X_test = [], []\n",
        "y_train, y_test = [], []\n",
        "\n",
        "for i in train_indices:\n",
        "    X_train.append(X[i])\n",
        "    y_train.append(y[i])\n",
        "\n",
        "for j in test_indices:\n",
        "    X_test.append(X[i])\n",
        "    y_test.append(y[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_function(filename, label):\n",
        "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
        "    Args:\n",
        "        filename: string representing path to image\n",
        "        label: 0/1 one-dimensional array of size N_LABELS\n",
        "    \"\"\"\n",
        "    # Read an image from a file\n",
        "    image_string = tf.io.read_file(filename)\n",
        "\n",
        "    # Decode it into a dense vector\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "\n",
        "    # Resize it to fixed shape\n",
        "    image_resized = tf.image.resize(image_decoded, [256, 256])\n",
        "\n",
        "    print(f\"{image_resized[0] = }\")\n",
        "\n",
        "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
        "    image_normalized = image_resized / 255.0\n",
        "    return image_normalized, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset(filenames, labels, is_training=True):\n",
        "    \"\"\"Load and parse dataset.\n",
        "    Args:\n",
        "        filenames: list of image paths\n",
        "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
        "        is_training: boolean to indicate training mode\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create a first dataset of file paths and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    # Parse and preprocess observations in parallel\n",
        "    dataset = dataset.map(parse_function)\n",
        "    \n",
        "    # if is_training == True:\n",
        "    #     # This is a small dataset, only load it once, and keep it in memory.\n",
        "    #     dataset = dataset.cache()\n",
        "    #     # Shuffle the data each buffer size\n",
        "    #     dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "        \n",
        "    # Batch the data for multiple steps\n",
        "    dataset = dataset.batch(256)\n",
        "    # # Fetch batches in the background while the model is training.\n",
        "    # dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image_resized[0] = <tf.Tensor 'strided_slice:0' shape=(256, 3) dtype=float32>\n",
            "image_resized[0] = <tf.Tensor 'strided_slice:0' shape=(256, 3) dtype=float32>\n"
          ]
        }
      ],
      "source": [
        "train_ds = create_dataset(X_train, y_train)\n",
        "val_ds = create_dataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display a target image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_image(idx, X, y):\n",
        "    img = X[idx]\n",
        "    plt.imshow(img)\n",
        "    plt.title(y[idx])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
        "    \n",
        "    Args:\n",
        "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
        "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
        "        thresh: probability value above which we predict positive\n",
        "        \n",
        "    Returns:\n",
        "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
        "    \"\"\"\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construct model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_model = Sequential()\n",
        "\n",
        "ds_model.add(Conv2D(filters = 28,\n",
        "    kernel_size = (3, 3),\n",
        "    input_shape = (256, 256, 3),\n",
        "    activation='relu',\n",
        "    padding = 'Same'))\n",
        "ds_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "ds_model.add(Conv2D(filters = 28,\n",
        "    kernel_size = (3, 3),\n",
        "    input_shape = (256, 256, 3),\n",
        "    activation='relu'))\n",
        "ds_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "ds_model.add(Conv2D(filters = 28,\n",
        "    kernel_size = (3, 3),\n",
        "    input_shape = (256, 256, 3),\n",
        "    activation='relu'))\n",
        "ds_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "ds_model.add(Flatten())\n",
        "\n",
        "ds_model.add(Dense(200, activation = 'relu'))\n",
        "ds_model.add(Dropout(0.2))\n",
        "\n",
        "ds_model.add(Dense(100, activation = 'relu'))\n",
        "ds_model.add(Dropout(0.1))\n",
        "\n",
        "ds_model.add(Dense(n_labels, activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt = K.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "ds_model.compile(optimizer=opt,\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[macro_f1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTycMqHSFgl"
      },
      "source": [
        "# Train model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4/4 [==============================] - 16s 3s/step - loss: 0.6888 - macro_f1: 0.0799 - val_loss: 0.6804 - val_macro_f1: 0.1176\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.6728 - macro_f1: 0.1062 - val_loss: 0.6681 - val_macro_f1: 0.1176\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.6573 - macro_f1: 0.1062 - val_loss: 0.6562 - val_macro_f1: 0.1176\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.6423 - macro_f1: 0.1062 - val_loss: 0.6447 - val_macro_f1: 0.1176\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.6277 - macro_f1: 0.1062 - val_loss: 0.6337 - val_macro_f1: 0.1176\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.6136 - macro_f1: 0.1062 - val_loss: 0.6231 - val_macro_f1: 0.1176\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.6000 - macro_f1: 0.1062 - val_loss: 0.6129 - val_macro_f1: 0.1176\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5869 - macro_f1: 0.1062 - val_loss: 0.6032 - val_macro_f1: 0.1176\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5744 - macro_f1: 0.1062 - val_loss: 0.5939 - val_macro_f1: 0.1176\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5623 - macro_f1: 0.1062 - val_loss: 0.5850 - val_macro_f1: 0.1176\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5507 - macro_f1: 0.1062 - val_loss: 0.5765 - val_macro_f1: 0.1176\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5395 - macro_f1: 0.1062 - val_loss: 0.5683 - val_macro_f1: 0.1176\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5288 - macro_f1: 0.1062 - val_loss: 0.5606 - val_macro_f1: 0.1176\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5186 - macro_f1: 0.1062 - val_loss: 0.5531 - val_macro_f1: 0.1176\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.5088 - macro_f1: 0.1062 - val_loss: 0.5460 - val_macro_f1: 0.1176\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.4994 - macro_f1: 0.1062 - val_loss: 0.5392 - val_macro_f1: 0.1176\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.4905 - macro_f1: 0.1062 - val_loss: 0.5327 - val_macro_f1: 0.1176\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 15s 3s/step - loss: 0.4819 - macro_f1: 0.1062 - val_loss: 0.5265 - val_macro_f1: 0.1176\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.4736 - macro_f1: 0.1062 - val_loss: 0.5206 - val_macro_f1: 0.1176\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4658 - macro_f1: 0.1062 - val_loss: 0.5149 - val_macro_f1: 0.1176\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4582 - macro_f1: 0.1062 - val_loss: 0.5094 - val_macro_f1: 0.1176\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4510 - macro_f1: 0.1062 - val_loss: 0.5042 - val_macro_f1: 0.1176\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4441 - macro_f1: 0.1062 - val_loss: 0.4991 - val_macro_f1: 0.1176\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4375 - macro_f1: 0.1062 - val_loss: 0.4943 - val_macro_f1: 0.1176\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4312 - macro_f1: 0.1062 - val_loss: 0.4896 - val_macro_f1: 0.1176\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4251 - macro_f1: 0.1062 - val_loss: 0.4852 - val_macro_f1: 0.1176\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4193 - macro_f1: 0.1062 - val_loss: 0.4809 - val_macro_f1: 0.1176\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4137 - macro_f1: 0.1062 - val_loss: 0.4767 - val_macro_f1: 0.1176\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4084 - macro_f1: 0.1062 - val_loss: 0.4727 - val_macro_f1: 0.1176\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4033 - macro_f1: 0.1062 - val_loss: 0.4689 - val_macro_f1: 0.1176\n"
          ]
        }
      ],
      "source": [
        "batchsize, epochs = 32, 30\n",
        "\n",
        "ds_history = ds_model.fit(train_ds,\n",
        "    epochs = epochs,\n",
        "    batch_size = batchsize,\n",
        "    validation_data = val_ds,\n",
        "    verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv0PFiSDSJr7"
      },
      "source": [
        "# View results\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 17), dtype=float64, numpy=\n",
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i = 0\n",
        "batch1 = None\n",
        "for batch in train_ds:\n",
        "    batch1 = batch\n",
        "    i += 1\n",
        "\n",
        "    if i == 1:\n",
        "        break\n",
        "\n",
        "\n",
        "batch1[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 199, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"sequential_4\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 256, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(32, 17) dtype=float64>]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18296/1032247351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_hat_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\notda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 199, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"sequential_4\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 256, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(32, 17) dtype=float64>]\n"
          ]
        }
      ],
      "source": [
        "y_hat_probs = ds_model.predict(batch1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPJmOGjzu1qhuay2vw9EljN",
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true,
      "name": "rainforest.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
