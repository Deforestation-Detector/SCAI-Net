{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8sALVSeZbDI"
      },
      "source": [
        "### We can construct a mosaic of nearby tiles using this method: https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/discussion/36738"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as K\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, Dense, Activation, BatchNormalization, Flatten\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import math\n",
        "import re\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.22.1'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.version.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "DS_BATCH_SIZE = 512\n",
        "MODEL_BATCH_SIZE = 32\n",
        "KERNEL_SIZE = 3\n",
        "IMG_DIMS = 256\n",
        "EPOCHS = 32\n",
        "DATA_PATH = 'data/train-jpg/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yztgcSUwSDwZ"
      },
      "source": [
        "# Preprocess data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtain Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'haze', 1: 'primary', 2: 'agriculture', 3: 'clear', 4: 'water', 5: 'habitation', 6: 'road', 7: 'cultivation', 8: 'slash_burn', 9: 'cloudy', 10: 'partly_cloudy', 11: 'conventional_mine', 12: 'bare_ground', 13: 'artisinal_mine', 14: 'blooming', 15: 'selective_logging', 16: 'blow_down'}\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv('data/train_v2.csv')\n",
        "\n",
        "curr_count = 0\n",
        "unique_labels = {}\n",
        "multihot = {}\n",
        "for line in train_data['tags'].values:\n",
        "    for label in line.split():\n",
        "        if label not in unique_labels:\n",
        "            unique_labels[label] = curr_count\n",
        "            curr_count += 1\n",
        "\n",
        "mapping = {}\n",
        "\n",
        "for k, v in unique_labels.items():\n",
        "    mapping[k] = np.zeros(len(unique_labels))\n",
        "    mapping[k][v] = 1\n",
        "\n",
        "n_labels = len(mapping)\n",
        "label2name = {v: k for k, v in unique_labels.items()}\n",
        "\n",
        "print(label2name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Head of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>haze primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>agriculture clear primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>clear primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>clear primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>agriculture clear habitation primary road</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>train_5</td>\n",
              "      <td>haze primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>train_6</td>\n",
              "      <td>agriculture clear cultivation primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>train_7</td>\n",
              "      <td>haze primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>train_8</td>\n",
              "      <td>agriculture clear cultivation primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>train_9</td>\n",
              "      <td>agriculture clear cultivation primary road</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>train_10</td>\n",
              "      <td>agriculture clear primary slash_burn water</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_name                                         tags\n",
              "0     train_0                                 haze primary\n",
              "1     train_1              agriculture clear primary water\n",
              "2     train_2                                clear primary\n",
              "3     train_3                                clear primary\n",
              "4     train_4    agriculture clear habitation primary road\n",
              "5     train_5                           haze primary water\n",
              "6     train_6  agriculture clear cultivation primary water\n",
              "7     train_7                                 haze primary\n",
              "8     train_8        agriculture clear cultivation primary\n",
              "9     train_9   agriculture clear cultivation primary road\n",
              "10   train_10   agriculture clear primary slash_burn water"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head(n = 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'haze': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'primary': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'agriculture': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'clear': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'water': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'habitation': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'road': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cultivation': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'slash_burn': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cloudy': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'partly_cloudy': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " 'conventional_mine': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
              " 'bare_ground': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " 'artisinal_mine': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
              " 'blooming': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
              " 'selective_logging': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
              " 'blow_down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Auxiliary Function for multi-hotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multihot(label, iter):\n",
        "    z = np.zeros(n_labels)\n",
        "    tokens = label.split(' ')\n",
        "\n",
        "    for k in range(len(tokens)):\n",
        "        z += mapping[tokens[k]]\n",
        "\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to grab the label from the filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_label(file_path):\n",
        "    x = file_path.split('\\\\')\n",
        "    file = x[-1]\n",
        "    idx = int(re.findall(r'\\d+', file)[0])\n",
        "\n",
        "    return train_data['tags'][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def readImage(filename, resize = [IMG_DIMS, IMG_DIMS]):\n",
        "    full_path = DATA_PATH + filename.decode(\"utf-8\") + '.jpg'\n",
        "    print(f\"{full_path = }\")\n",
        "    img = Image.open(full_path).convert(\"RGB\")\n",
        "    img = np.asarray(img) / 255\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    img = tf.image.resize(img, resize)\n",
        "\n",
        "    print(f\"{img = }\")\n",
        "\n",
        "    return img\n",
        "\n",
        "def parse_function(filename, label):\n",
        "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
        "    Args:\n",
        "        filename: string representing path to image\n",
        "        label: 0/1 one-dimensional array of size N_LABELS\n",
        "    \"\"\"\n",
        "\n",
        "    # print(f\"{DATA_PATH + filename + '.jpg'}\")\n",
        "\n",
        "    # image_string = tf.io.read_file(DATA_PATH + filename + '.jpg')\n",
        "    # image_decoded = tf.image.decode_jpeg(image_string)\n",
        "    # image_resized = tf.image.resize(image_decoded, resize)\n",
        "    # image_normalized = image_resized / 255.0\n",
        "\n",
        "    # print(f\"{image_normalized = }\")\n",
        "\n",
        "    img = tf.numpy_function(readImage, [filename], tf.float32)\n",
        "\n",
        "    # # print(f\"{img = }\")\n",
        "\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full_path = 'data/train-jpg/train_0.jpg'\n",
            "img = <tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n",
            "array([[[0.38039216, 0.4392157 , 0.4117647 ],\n",
            "        [0.36862746, 0.4392157 , 0.4       ],\n",
            "        [0.36862746, 0.4392157 , 0.39607844],\n",
            "        ...,\n",
            "        [0.38039216, 0.4392157 , 0.40392157],\n",
            "        [0.38039216, 0.43529412, 0.40392157],\n",
            "        [0.37254903, 0.42745098, 0.40392157]],\n",
            "\n",
            "       [[0.36862746, 0.42745098, 0.4       ],\n",
            "        [0.36078432, 0.42745098, 0.39215687],\n",
            "        [0.36078432, 0.43137255, 0.39215687],\n",
            "        ...,\n",
            "        [0.37254903, 0.43137255, 0.4       ],\n",
            "        [0.37254903, 0.42745098, 0.40392157],\n",
            "        [0.36862746, 0.42352942, 0.40392157]],\n",
            "\n",
            "       [[0.3529412 , 0.41568628, 0.39215687],\n",
            "        [0.34901962, 0.41568628, 0.3882353 ],\n",
            "        [0.3529412 , 0.42352942, 0.3882353 ],\n",
            "        ...,\n",
            "        [0.3647059 , 0.41960785, 0.39607844],\n",
            "        [0.36078432, 0.41960785, 0.4       ],\n",
            "        [0.35686275, 0.41960785, 0.4       ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0.3647059 , 0.43529412, 0.42352942],\n",
            "        [0.36862746, 0.43529412, 0.41960785],\n",
            "        [0.36862746, 0.4392157 , 0.41960785],\n",
            "        ...,\n",
            "        [0.3529412 , 0.43137255, 0.41568628],\n",
            "        [0.34117648, 0.42352942, 0.41568628],\n",
            "        [0.32156864, 0.40392157, 0.4       ]],\n",
            "\n",
            "       [[0.3764706 , 0.42745098, 0.41960785],\n",
            "        [0.37254903, 0.43137255, 0.41960785],\n",
            "        [0.36862746, 0.43137255, 0.41568628],\n",
            "        ...,\n",
            "        [0.36078432, 0.43529412, 0.4117647 ],\n",
            "        [0.34901962, 0.42352942, 0.4117647 ],\n",
            "        [0.3254902 , 0.40392157, 0.39607844]],\n",
            "\n",
            "       [[0.38431373, 0.42352942, 0.41568628],\n",
            "        [0.3764706 , 0.42352942, 0.4117647 ],\n",
            "        [0.36862746, 0.42352942, 0.4117647 ],\n",
            "        ...,\n",
            "        [0.36862746, 0.43137255, 0.40784314],\n",
            "        [0.3529412 , 0.42352942, 0.40392157],\n",
            "        [0.3254902 , 0.4       , 0.38431373]]], dtype=float32)>\n",
            "i[0][0, 0] = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.38039216, 0.4392157 , 0.4117647 ], dtype=float32)>\n",
            "full_path = 'data/train-jpg/train_1.jpg'\n",
            "img = <tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n",
            "array([[[0.18431373, 0.23921569, 0.21568628],\n",
            "        [0.2       , 0.24705882, 0.21960784],\n",
            "        [0.21960784, 0.2627451 , 0.23137255],\n",
            "        ...,\n",
            "        [0.18039216, 0.21176471, 0.1882353 ],\n",
            "        [0.20392157, 0.22745098, 0.20784314],\n",
            "        [0.23137255, 0.2509804 , 0.23137255]],\n",
            "\n",
            "       [[0.19607843, 0.24313726, 0.22352941],\n",
            "        [0.20392157, 0.2509804 , 0.22745098],\n",
            "        [0.21960784, 0.2627451 , 0.23137255],\n",
            "        ...,\n",
            "        [0.16078432, 0.2       , 0.18039216],\n",
            "        [0.16862746, 0.21176471, 0.1882353 ],\n",
            "        [0.1882353 , 0.23529412, 0.20392157]],\n",
            "\n",
            "       [[0.20784314, 0.24705882, 0.23529412],\n",
            "        [0.21176471, 0.25490198, 0.23529412],\n",
            "        [0.21960784, 0.2627451 , 0.23529412],\n",
            "        ...,\n",
            "        [0.16078432, 0.19215687, 0.18431373],\n",
            "        [0.14901961, 0.2       , 0.18039216],\n",
            "        [0.15294118, 0.21568628, 0.1882353 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0.36078432, 0.38431373, 0.2901961 ],\n",
            "        [0.37254903, 0.39215687, 0.30588236],\n",
            "        [0.3882353 , 0.40392157, 0.32156864],\n",
            "        ...,\n",
            "        [0.2901961 , 0.34117648, 0.28235295],\n",
            "        [0.32156864, 0.36078432, 0.3019608 ],\n",
            "        [0.34901962, 0.3764706 , 0.31764707]],\n",
            "\n",
            "       [[0.3529412 , 0.38039216, 0.28235295],\n",
            "        [0.36862746, 0.3882353 , 0.3019608 ],\n",
            "        [0.3882353 , 0.4       , 0.3254902 ],\n",
            "        ...,\n",
            "        [0.31764707, 0.35686275, 0.2901961 ],\n",
            "        [0.34901962, 0.3764706 , 0.30980393],\n",
            "        [0.37254903, 0.39215687, 0.3254902 ]],\n",
            "\n",
            "       [[0.34901962, 0.3764706 , 0.2784314 ],\n",
            "        [0.36862746, 0.38431373, 0.3019608 ],\n",
            "        [0.3882353 , 0.4       , 0.3254902 ],\n",
            "        ...,\n",
            "        [0.3372549 , 0.36078432, 0.2901961 ],\n",
            "        [0.3647059 , 0.38039216, 0.3137255 ],\n",
            "        [0.3882353 , 0.39607844, 0.32941177]]], dtype=float32)>\n",
            "i[0][0, 0] = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.18431373, 0.23921569, 0.21568628], dtype=float32)>\n"
          ]
        }
      ],
      "source": [
        "# first pass, construct a list of image strips\n",
        "\n",
        "file_paths = train_data['image_name'].values\n",
        "labels_string = train_data['tags'].values\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((file_paths, labels_string))\n",
        "ds_train = ds_train.map(parse_function)\n",
        "\n",
        "for i in ds_train.take(2):\n",
        "    print(f\"{i[0][0, 0] = }\")\n",
        "\n",
        "# dataset_length = len(os.listdir(DATA_PATH))\n",
        "\n",
        "# images_ds = tf.data.Dataset.list_files(f\"{DATA_PATH}*\", shuffle = False)\n",
        "# steps = math.floor(0.05 * dataset_length)\n",
        "\n",
        "# X = []\n",
        "# y = np.zeros(shape = (dataset_length, n_labels))\n",
        "# for iter, filename in enumerate(os.listdir(DATA_PATH)[:dataset_length]):\n",
        "#     label = get_label(filename)\n",
        "#     y[iter] = multihot(label, iter)\n",
        "#     X.append(DATA_PATH + filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into a test and train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(images_ds) = 40479, len(X_train) = 32383, len(X_test) = 8096\n"
          ]
        }
      ],
      "source": [
        "train_length = math.floor(0.8 * dataset_length)\n",
        "X_train, X_test = X[:train_length], X[train_length:]\n",
        "y_train_bin, y_test_bin = y[:train_length], y[train_length:]\n",
        "# X_train, X_test = images_ds.take(train_length), images_ds.skip(train_length)\n",
        "\n",
        "print(f\"{len(images_ds) = }, {len(X_train) = }, {len(X_test) = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting the first file for testing purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filename = 'data/train-jpg/train_0.jpg'\n"
          ]
        }
      ],
      "source": [
        "test_file = None\n",
        "\n",
        "for filename in X_train[:1]:\n",
        "    print(f\"{filename = }\")\n",
        "    test_file = filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to encode a 2-tuple of tensors from the name of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_function(filename, label, resize = [IMG_DIMS, IMG_DIMS]):\n",
        "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
        "    Args:\n",
        "        filename: string representing path to image\n",
        "        label: 0/1 one-dimensional array of size N_LABELS\n",
        "    \"\"\"\n",
        "\n",
        "    # image_string = tf.io.read_file(filename)\n",
        "    # image_decoded = tf.image.decode_jpeg(image_string)\n",
        "    # image_resized = tf.image.resize(image_decoded, [256, 256])\n",
        "    # image_normalized = image_resized / 255.0\n",
        "\n",
        "    print(f\"{tf.convert_to_tensor(filename).numpy() = }\")\n",
        "    img = Image.open(filename).convert(\"RGB\")\n",
        "    img = np.asarray(img) / 255\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    img = tf.image.resize(img, resize)\n",
        "\n",
        "    print(f\"{img = }\")\n",
        "\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset(filenames, labels):\n",
        "    \"\"\"Load and parse dataset.\n",
        "    Args:\n",
        "        filenames: list of image paths\n",
        "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
        "        is_training: boolean to indicate training mode\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a first dataset of file paths and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "    # Parse and preprocess observations in parallel\n",
        "    dataset = dataset.map(parse_function)\n",
        "        \n",
        "    # Batch the data for multiple steps\n",
        "    dataset = dataset.batch(DS_BATCH_SIZE)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\notda\\AppData\\Local\\Temp/ipykernel_29584/1512394455.py\", line 13, in parse_function  *\n        print(f\"{tf.convert_to_tensor(filename).numpy() = }\")\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29584/923154295.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29584/3433374141.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(filenames, labels)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Parse and preprocess observations in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparse_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Batch the data for multiple steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2002\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m   2003\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m-> 2004\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5453\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5454\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5455\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   5456\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5457\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4533\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4534\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3242\u001b[0m          \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \"\"\"\n\u001b[1;32m-> 3244\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3245\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3210\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3390\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3391\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3392\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3393\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4508\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   4509\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4510\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4511\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4438\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4440\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4441\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4442\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\notda\\AppData\\Local\\Temp/ipykernel_29584/1512394455.py\", line 13, in parse_function  *\n        print(f\"{tf.convert_to_tensor(filename).numpy() = }\")\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n"
          ]
        }
      ],
      "source": [
        "train_ds = create_dataset(X_train, y_train_bin)\n",
        "val_ds = create_dataset(X_test, y_test_bin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x, y = (<tf.Tensor: shape=(512, 256, 256, 3), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(512, 17), dtype=float64, numpy=\n",
            "array([[1., 1., 0., ..., 0., 0., 0.],\n",
            "       [0., 1., 1., ..., 0., 0., 0.],\n",
            "       [0., 1., 1., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 1., 0., ..., 0., 0., 0.],\n",
            "       [0., 1., 0., ..., 0., 0., 0.],\n",
            "       [0., 1., 0., ..., 0., 0., 0.]])>)\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_ds:\n",
        "    print(f\"{x, y = }\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display a target image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_image(idx, X, y):\n",
        "    img = X[idx]\n",
        "    print(f\"{img = }\")\n",
        "    plt.imshow(img)\n",
        "    plt.title('rinkydinky')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24152/162716564.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# show_image(0, batch[0], batch[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m240\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7105\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7107\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/"
          ]
        }
      ],
      "source": [
        "batch = None\n",
        "\n",
        "for e in X_train:\n",
        "    batch = e\n",
        "    break\n",
        "\n",
        "# show_image(0, batch[0], batch[1])\n",
        "batch[0][240]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{batch[0] = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
        "    \n",
        "    Args:\n",
        "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
        "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
        "        thresh: probability value above which we predict positive\n",
        "        \n",
        "    Returns:\n",
        "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
        "    \"\"\"\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construct model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_model = Sequential()\n",
        "\n",
        "ds_model.add(Conv2D(filters = 28,\n",
        "    kernel_size = (KERNEL_SIZE, KERNEL_SIZE),\n",
        "    input_shape = (IMG_DIMS, IMG_DIMS, 3),\n",
        "    activation='relu',\n",
        "    padding = 'Same'))\n",
        "ds_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "ds_model.add(Conv2D(filters = 28,\n",
        "    kernel_size = (KERNEL_SIZE, KERNEL_SIZE),\n",
        "    activation='relu'))\n",
        "ds_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "ds_model.add(Conv2D(filters = 28,\n",
        "    kernel_size = (KERNEL_SIZE, KERNEL_SIZE),\n",
        "    activation='relu'))\n",
        "ds_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "ds_model.add(Flatten())\n",
        "\n",
        "ds_model.add(Dense(200, activation = 'relu'))\n",
        "ds_model.add(Dropout(0.2))\n",
        "\n",
        "ds_model.add(Dense(100, activation = 'relu'))\n",
        "ds_model.add(Dropout(0.1))\n",
        "\n",
        "ds_model.add(Dense(n_labels, activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt = K.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "ds_model.compile(optimizer=opt,\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[macro_f1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTycMqHSFgl"
      },
      "source": [
        "# Train model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_history = ds_model.fit(train_ds,\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = MODEL_BATCH_SIZE,\n",
        "    validation_data = val_ds,\n",
        "    verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv0PFiSDSJr7"
      },
      "source": [
        "# View results\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "batch1 = None\n",
        "for batch in train_ds:\n",
        "    batch1 = batch\n",
        "    i += 1\n",
        "\n",
        "    if i == 1:\n",
        "        break\n",
        "\n",
        "# NOTE: Batch1 is a TUPLE, not a tensor.\n",
        "# It's comprised of two separate tensors, where the first\n",
        "# element is the set of feature tensors of dimension 512x256x256x3\n",
        "# because each batch is comprised of 512 elements, each being\n",
        "# 256x256x3 images.\n",
        "# The second element in the tuple is the set of multihot encodings\n",
        "\n",
        "print(f\"{batch1 = }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_hat_probs = ds_model.predict(batch1[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_hat_probs[240]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y[240]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPJmOGjzu1qhuay2vw9EljN",
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true,
      "name": "rainforest.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
